{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonthlySVD\n",
    "This code shows as simple way to:\n",
    "- Read in a .nc file using xarray\n",
    "- Generate montly anomalies from climatology of monthly means\n",
    "- detrend data by grid cell\n",
    "- calculate singular value decomposition by grid cell, dim = month * year\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import cf_units as cf\n",
    "import esmlab\n",
    "\n",
    "# some resources for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy  # Try scipy to detrend\n",
    "import segment as sg\n",
    "from ctsm_py import utils\n",
    "from scipy import signal\n",
    "\n",
    "# import cartopy\n",
    "# import cartopy.crs as ccrs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# supress Runtime warnings that let you know when code isn't too efficiently\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# TODO, turn warmings back on with code below?\n",
    "# warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Read in data ----"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"GPP\", \"TBOT\", \"SOILLIQ\"]  # TWS not available for CESM1 / CLM4\n",
    "\n",
    "# --- CESM2 hist (1 ensemble member) ---\n",
    "# model = 'CESM2_hist1'\n",
    "# pattern = '/glade/collections/cdg/timeseries-cmip6/b.e21.BHIST.f09_g17.CMIP6-historical.001/' \\\n",
    "#    'lnd/proc/tseries/month_1/b.e21.BHIST.f09_g17.CMIP6-historical.001.clm2.h0.{var}.185001-201412.nc'\n",
    "\n",
    "# --- CESM1-LENS hist (1 ensemble member) ---\n",
    "model = \"CESM1_LENS1\"\n",
    "pattern = (\n",
    "    \"/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/lnd/proc/tseries/monthly/\"\n",
    "    \"{var}/b.e11.B20TRC5CNBDRD.f09_g16.001.clm2.h0.{var}.185001-200512.nc\"\n",
    ")\n",
    "\n",
    "# --- CLM5 w/ GSWP3 ---\n",
    "# model = 'CLM5_GSWP3'\n",
    "# pattern = '/glade/p/cgd/tss/people/oleson/CLM_LAND_ONLY_RELEASE/CLM5/clm50_r270_1deg_GSWP3V1_iso_newpopd_hist/'\\\n",
    "#    'lnd/proc/tseries/month_1/clm50_r270_1deg_GSWP3V1_iso_newpopd_hist.clm2.h0.{var}.185001-201412.nc'\n",
    "\n",
    "# --- CLM45 w/ GSWP3 ---\n",
    "# model ='CLM45_GSWP3'\n",
    "# pattern = '/glade/p/cgd/tss/people/oleson/CLM_LAND_ONLY_RELEASE/CLM4.5/clm45_r270_1deg_GSWP3V1_hist/'\\\n",
    "#    'lnd/proc/tseries/month_1/clm45_r270_1deg_GSWP3V1_hist.clm2.h0.{var}.185001-201412.nc'\n",
    "\n",
    "# --- CLM40 w/ GSWP3 ---\n",
    "# NO TWS output!!\n",
    "# model ='CLM4_GSWP3'\n",
    "# pattern = '/glade/p/cgd/tss/people/oleson/CLM_LAND_ONLY_RELEASE/CLM4/clm40_r270_1deg_GSWP3v1_CMIP6_hist/'\\\n",
    "#    'lnd/proc/tseries/month_1/clm40_r270_1deg_GSWP3v1_CMIP6_hist.clm2.h0.{var}.185001-201412.nc'\n",
    "\n",
    "# --- Danica's no crop run ---\n",
    "# model = 'CLM5_GSWP3_noCrop'\n",
    "# pattern = '/glade/p/cesm/lmwg_dev/dll/CLM5GSWP3_NoCrop/SingleVarTimeFiles/'\\\n",
    "#    'clm50_r267_1deg_GSWP3V1_iso_hist_nocrop_transientfix.clm2.h0.{var}.185001-201012.nc'\n",
    "\n",
    "file = [pattern.format(var=var) for var in variables]\n",
    "print(file[1])\n",
    "\n",
    "# would be nice to just do this in a loop...\n",
    "var = variables  # redundant, but 'var' is used more later on...\n",
    "ds0 = utils.time_set_mid(xr.open_dataset(file[0], decode_times=True), \"time\")\n",
    "ds1 = utils.time_set_mid(xr.open_dataset(file[1], decode_times=True), \"time\")\n",
    "ds2 = utils.time_set_mid(xr.open_dataset(file[2], decode_times=True), \"time\")\n",
    "if variables[2] == \"SOILLIQ\":\n",
    "    dim_lev = ds2[var[2]].dims[1]\n",
    "    ds2 = ds2.sum(dim=dim_lev)\n",
    "\n",
    "ds = xr.merge([ds0, ds1], compat=\"override\")\n",
    "ds[var[2]] = ds2[var[2]]\n",
    "ds[\"TBOT\"] = ds.TBOT - 273.15\n",
    "area = ds.area\n",
    "landfrac = ds.landfrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JH suggestions, trouble shooting to see if this is why loop below fails...\n",
    "# reading data this way gets rid of chunk sizes\n",
    "\n",
    "# utils.time_set_mid corrects dates, to get Jan of first year\n",
    "# combine needed for newer xarray versions\n",
    "# ds = utils.time_set_mid(xr.open_mfdataset(files, combine='by_coords', decode_times=True), 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 25\n",
    "months = years * 12\n",
    "tlat = 60  # 60#-5 #  #46  #-3\n",
    "tlon = 240  # 240#300 #262 #280\n",
    "ds_test = ds.sel(\n",
    "    lat=tlat, lon=tlon, method=\"nearest\"\n",
    ")  # select a single grid point to work with first\n",
    "ds_last = ds_test.isel(time=slice(-months, None))\n",
    "dataset = ds_last.get(var)\n",
    "ds_ann = dataset.apply(utils.weighted_annual_mean)\n",
    "plt.plot(ds_last.time[\"time\"], ds_last.get(var[2]), \"-\")\n",
    "plt.plot(ds_ann.time[\"time\"], ds_ann.get(var[2]), \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate climatology and anomalies"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esmlab anomaly function needs time_bounds in the dataset!\n",
    "# test to see of climatology and anomaly functions see to work appropriately?\n",
    "# resample monthly data to seasonal\n",
    "ds_clim = esmlab.core.climatology(ds_last, freq=\"mon\", time_coord_name=\"time\")\n",
    "ds_anom = esmlab.core.anomaly(ds_last, clim_freq=\"mon\", time_coord_name=\"time\")\n",
    "ds_detrend = ds_anom.get(var).map(\n",
    "    signal.detrend\n",
    ")  # still need to use get so detrend works\n",
    "ds_detrend_season = ds_detrend.resample(time=\"QS-DEC\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at seasonal\n",
    "plt.figure(figsize=[16, 5])\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, (1 + i))\n",
    "    plt.plot(ds_last.time[\"time.month\"], ds_last.get(var[i]), \"o\")\n",
    "    plt.plot(ds_clim.time[\"time.month\"], ds_clim.get(var[i]), \"-\", lw=3)\n",
    "    plt.plot(ds_anom.time[\"time.month\"], ds_anom.get(var[i]), \"o\")\n",
    "    plt.plot(ds_anom.time[\"time.month\"], ds_detrend.get(var[i]), \"*\")\n",
    "    plt.axhline(0, color=\"black\", lw=1)\n",
    "    plt.ylabel(var[i] + \", \" + model + \" lat= \" + str(tlat))\n",
    "    plt.xlabel(\"month\")\n",
    "    if i == 1:\n",
    "        plt.legend(labels=(\"raw\", \"clim\", \"anom\", \"detrend_anom\"), frameon=False)\n",
    "plt.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaonal GPP anomalies seem to show a similar pattern to TWS in CLM4.5 \n",
    "#### Stonger temperature sensitivity w/ CLM5?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "color = \"tab:green\"\n",
    "ax1.set_xlabel(\"time\")\n",
    "ax1.set_ylabel(var[0] + \" \" + model, color=color)\n",
    "ax1.plot(ds_detrend_season.time, ds_detrend_season.get(var[0]), \"-\", color=color)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = \"tab:red\"\n",
    "ax2.set_ylabel(var[1], color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(ds_detrend_season.time, ds_detrend_season.get(var[1]), \"--\", color=color)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = \"tab:blue\"\n",
    "ax3.set_ylabel(var[2], color=color)  # we already handled the x-label with ax1\n",
    "ax3.plot(ds_detrend_season.time, ds_detrend_season.get(var[2]), \"-\", color=color)\n",
    "ax3.tick_params(axis=\"y\", labelcolor=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Do we need to detrend data?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_anom.time[\"time\"], ds_anom.get(var[0]), \"-\", c=\"green\")\n",
    "plt.plot(ds_anom.time[\"time\"], ds_detrend.get(var[0]), \"-\", c=\"red\")\n",
    "plt.ylabel(var[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start singular vector decomposition\n",
    "#### This is from Gretchen's CLM_SVD code "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig. mon x year\n",
    "iavmatrix = np.zeros([3, 12, years])\n",
    "for i in range(len(var)):\n",
    "    for iyr in range(years):\n",
    "        iavmatrix[i, 0:12, iyr] = ds_detrend.get(var[i])[\n",
    "            iyr * 12 : (iyr + 1) * 12\n",
    "        ]  # reshape timeseries vector into a matrix (year x month)\n",
    "\n",
    "plt.figure(figsize=[16, 5])\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, (1 + i))\n",
    "    plt.ylabel(var[i] + \" detrended anomalies, all years\")\n",
    "    plt.plot(iavmatrix[i, :, :])\n",
    "\n",
    "plt.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try using SVD_tools.py\n",
    "This code was provided by Gretchen Keppel-Aleks & initially developed by Zach Butterfield\n",
    "\n",
    "Further modified by W. Wieder to accept matrix when years > 12 (nmonths)\n",
    "- Decompose function accepts a month * year matrix and calculates vectors (dimensions = years,months) and weights (years,years)  \n",
    "- Redistribution function takes vectors, weights, and matrix to calculate theta (years), & varfrac (years)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SVD_tools as svd\n",
    "\n",
    "# assign the 2-D matrix (month x year) to decompose\n",
    "# Call the function to calculate the singular vectors and their annual weights\n",
    "vectors, weights = svd.decompose(iavmatrix[0, :, :])\n",
    "theta, varfrac = svd.calc_redistribution(vectors, weights, iavmatrix[0, :, :])\n",
    "print(vectors.shape)  # (nyears , nmonths)\n",
    "print(weights.shape)\n",
    "# print(ds_detrend_season) #(101 time slices, needs to be reshaped to nyears*season)\n",
    "\n",
    "# ----- run all stats on hydrologic year in SH -----\n",
    "mm2plot = ds_clim.get(var[0])\n",
    "idx = [6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5]\n",
    "\n",
    "if tlat < 0:\n",
    "    mm2plot = mm2plot[idx]\n",
    "    for i in range(2):  # just focus on first two vectors\n",
    "        vectors[i, :] = vectors[i, :][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also try spearman's rank correlation\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "s = np.zeros(years)\n",
    "for i in range(years):\n",
    "    s[i] = spearmanr(mm2plot, vectors[i, :])[0]\n",
    "print(\"Spearmans correlation SV1 vectors ~ Climatology: %.3f\" % s[0])\n",
    "print(\"Spearmans correlation SV2 vectors ~ Climatology: %.3f\" % s[1])\n",
    "\n",
    "# also try weighting correlation based on monthly contribution to annual flux\n",
    "wgt = mm2plot / mm2plot.sum()\n",
    "\n",
    "\n",
    "def cov(x, y, w):\n",
    "    \"\"\"Weighted Covariance\"\"\"\n",
    "    return np.sum(\n",
    "        w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))\n",
    "    ) / np.sum(w)\n",
    "\n",
    "\n",
    "def corr(x, y, w):\n",
    "    \"\"\"Weighted Correlation\"\"\"\n",
    "    return cov(x, y, w) / np.sqrt(cov(x, x, w) * cov(y, y, w))\n",
    "\n",
    "\n",
    "r = np.zeros(years)\n",
    "for i in range(years):\n",
    "    r[i] = corr(mm2plot.values, vectors[i, :], wgt.values)\n",
    "\n",
    "print(\"weighted correlation SV1 vectors ~ Climatology: %.3f\" % r[0])\n",
    "print(\"weighted correlation SV2 vectors ~ Climatology: %.3f\" % r[1])\n",
    "## for our generic tropical plot, SV1 = early shift & SV2 = amplification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can correlations between climatology and Vectors tell us someting about the nature of relationships?\n",
    "#### In this example SV? shows higher correlation with climatology of GPP"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add logicals to flip vectors and weights if:\n",
    "# 1a) amplificaiton vector and correlation with climatology < 0\n",
    "# 2) phase shift vector and minVector comes before maxVector\n",
    "# 1b) amplificaiton vector and sum(vectors)<0 [this corrects issues in the tropics]\n",
    "\n",
    "flip_corr = s < 0  # here using spearmans\n",
    "\n",
    "min_vector = np.argmin(vectors, axis=1)\n",
    "max_vector = np.argmax(vectors, axis=1)\n",
    "flip_vector = min_vector < max_vector\n",
    "print(flip_vector)\n",
    "\n",
    "# only flip corr where amplificaiton, vector where phase\n",
    "if np.absolute(s[0]) < np.absolute(s[1]):\n",
    "    flip_corr[0] = False\n",
    "    flip_vector[1] = False\n",
    "else:\n",
    "    flip_corr[1] = False\n",
    "    flip_vector[0] = False\n",
    "\n",
    "flip_vector = flip_corr + flip_vector\n",
    "print(flip_vector)\n",
    "\n",
    "for i in range(years):\n",
    "    if flip_vector[i] == True:\n",
    "        vectors[i, :] = -vectors[i, :]\n",
    "        weights[i, :] = -weights[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_amp = np.mean(vectors, axis=1) < 0.0\n",
    "if np.absolute(s[0]) < np.absolute(s[1]):\n",
    "    flip_amp[0] = False\n",
    "\n",
    "print(flip_amp)\n",
    "for i in range(years):\n",
    "    if flip_amp[i] == True:\n",
    "        vectors[i, :] = -vectors[i, :]\n",
    "        weights[i, :] = -weights[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot First and Second Singular Vectors\n",
    "# TODO This is a pretty complicated plotting method that could likely be improved & made into a function\n",
    "# That said, the it makes nice, clear plots... we'll keep it for now\n",
    "\n",
    "fig = plt.figure(4, figsize=(3, 4.5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "month = np.arange(12)\n",
    "barwidth = 0.2\n",
    "ymax = np.max(vectors)\n",
    "vectors2plot = vectors * 7000  # This is a totally arbitrary scale factor,\n",
    "#                            decided by me, to make figures look good!\n",
    "ax1.plot(\n",
    "    np.array(month) + 2 * barwidth,\n",
    "    mm2plot / (4 * max(mm2plot)) - 0.05,\n",
    "    \"darkgray\",\n",
    "    linewidth=3,\n",
    ")\n",
    "ax1.plot(\n",
    "    np.array(month) + 2 * barwidth,\n",
    "    mm2plot / (4 * max(mm2plot)) + 0.35,\n",
    "    \"darkgray\",\n",
    "    linewidth=3,\n",
    ")\n",
    "ax1.plot(\n",
    "    np.array(month) + 2 * barwidth, vectors2plot[0, :] * 0.75 + 0.35, \"r-\", linewidth=3\n",
    ")\n",
    "ax1.plot(\n",
    "    np.array(month) + 2 * barwidth, vectors2plot[1, :] * 0.75 - 0.05, \"b-\", linewidth=3\n",
    ")\n",
    "ax1.axhline(-0.05, color=\"black\")\n",
    "ax1.axhline(0.35, color=\"black\")\n",
    "ax1.set_xticks(np.array(month) + 2 * barwidth)\n",
    "ax1.set_xticklabels([\"Jan\", \"\", \"\", \"Apr\", \"\", \"\", \"Jul\", \"\", \"\", \"Oct\", \"\", \"\"])\n",
    "ax1.set_yticks([-0.05, 0.35])\n",
    "ax1.set_yticklabels([\"SV2\", \"SV1\"])\n",
    "ax1.set_ylim([-0.2, 0.65])\n",
    "ax1.text(\n",
    "    0.5,\n",
    "    0.50,\n",
    "    str(int(varfrac[0] * 100))\n",
    "    + \"%\\n$\\Theta$=\"\n",
    "    + str(\"%.2f\" % theta[0])\n",
    "    + \"\\n\"\n",
    "    + str(\"rank cor %.2f\" % s[0]),\n",
    ")\n",
    "ax1.text(\n",
    "    0.5,\n",
    "    0.10,\n",
    "    str(int(varfrac[1] * 100))\n",
    "    + \"%\\n$\\Theta$=\"\n",
    "    + str(\"%.2f\" % theta[1])\n",
    "    + \"\\n\"\n",
    "    + str(\"rank cor %.2f\" % s[1]),\n",
    ")\n",
    "\n",
    "ax1.title.set_text(model + \" \" + var[0])\n",
    "ax1.text(4, -0.29, \"Month\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SVD_\" + model + \"_ex_\" + var[0] + \".eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do weights from SV correlate with seasonal climate anomalies?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, there must be a better way to reshape data, but I can't get group by to work\n",
    "season_group = ds_detrend_season.groupby(\"time.month\")\n",
    "\n",
    "# reshape timeseries vector into a matrix (season x year)\n",
    "# Omits the last year, which only has data for Dec\n",
    "group_0 = np.zeros([4, years])\n",
    "group_1 = np.zeros([4, years])\n",
    "group_2 = np.zeros([4, years])\n",
    "\n",
    "# Reshape timeseries vector into a matrix (season x year)\n",
    "# this could also be done with np.reshape...\n",
    "for iyr in range(years):\n",
    "    group_0[0:4, iyr] = ds_detrend_season.get(var[0])[(iyr * 4) : (iyr * 4) + 4]\n",
    "    group_1[0:4, iyr] = ds_detrend_season.get(var[1])[(iyr * 4) : (iyr * 4) + 4]\n",
    "    group_2[0:4, iyr] = ds_detrend_season.get(var[2])[(iyr * 4) : (iyr * 4) + 4]\n",
    "\n",
    "# Plot seasonal anomalies by year\n",
    "plt.plot(group_0[0,], \"-\", color=\"green\")  # DJF\n",
    "plt.plot(group_0[1,], \"-\", color=\"red\")  # MAM\n",
    "plt.plot(group_0[2,], \"-\", color=\"blue\")  # JJA\n",
    "plt.plot(group_0[3,], \"-\", color=\"black\")\n",
    "# SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlations with climate anomalies ---\n",
    "print(\"correlation of sv weights & with seasonal anomalies\")\n",
    "quarter = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]  # can coords be a string?\n",
    "\n",
    "# Here using weights that were flipped like the vectors\n",
    "for h in range(2):  # vectors\n",
    "    print(\"  -------- SV\" + str(h + 1) + \" -----------\")\n",
    "    for i in range(4):  # season\n",
    "        r0 = corr(weights[h, :], group_0[i, :], np.ones(years))\n",
    "        r1 = corr(weights[h, :], group_1[i, :], np.ones(years))\n",
    "        r2 = corr(weights[h, :], group_2[i, :], np.ones(years))\n",
    "        print(\n",
    "            var[0] + \" weights SV ~ \" + var[0] + \" \" + quarter[i] + \" anom. : %.3f\" % r0\n",
    "        )\n",
    "        print(\n",
    "            var[0] + \" weights SV ~ \" + var[1] + \" \" + quarter[i] + \" anom. : %.3f\" % r1\n",
    "        )\n",
    "        print(\n",
    "            var[0] + \" weights SV ~ \" + var[2] + \" \" + quarter[i] + \" anom. : %.3f\" % r2\n",
    "        )\n",
    "        print(\"--\")\n",
    "\n",
    "# once we arrange the vectors correctly\n",
    "# phase shift (SV1),  + correlated with MAM TBOT anomalies, - correlated with SON TWS\n",
    "# amplification(SV2), + correlated with JJA TWS anomalies\n",
    "# this pattern of Phase ~ TBOT & Amp ~ TWS holds for CLM4.5 too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the NH gridcell,\n",
    "# weights for SV1 of GPP = the inverse as MAM GPP anomalies\n",
    "# weights for SV2 of GPP = the inverse as JJA GPP anomalies\n",
    "# in SH\n",
    "# SV1 weights - corr w/ GPP & TWS in JJA and SON\n",
    "# SV2 weights + corr w/ GPP in MAM, not climate\n",
    "\n",
    "s2y = 3600 * 24 * 365\n",
    "if tlat >= 0:\n",
    "    plt.plot(group_0[1, :] * s2y, (weights[0, :]), \"*\", color=\"red\")\n",
    "    plt.plot(group_0[2, :] * s2y, (weights[1, :]), \"*\", color=\"blue\")\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        labels=(\n",
    "            \"SV1 weights ~ MAM \" + var[0] + \" anomalies\",\n",
    "            \"SV2 weights ~ JJA \" + var[0] + \" anomalies\",\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    plt.plot(group_0[3, :] * s2y, (weights[0, :]), \"*\", color=\"red\")\n",
    "    plt.plot(group_0[1, :] * s2y, (weights[1, :]), \"*\", color=\"blue\")\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        labels=(\n",
    "            \"SV1 weights ~ SON \" + var[0] + \" anomalies\",\n",
    "            \"SV2 weights ~ MAM \" + var[0] + \" anomalies\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "plt.xlabel(var[0] + \" seasonal anomalies (gC/m2/y)\")\n",
    "plt.ylabel(var[0] + \" SV weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "# let's reverse weights for clarity (vectors are reversed above)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "color = \"red\"\n",
    "if tlat >= 0:\n",
    "    x = group_1[1, :]  # MAM TBOT\n",
    "    xlab = \"MAM \" + var[1] + \" anomalies\"\n",
    "else:\n",
    "    x = group_2[3, :]  # SON TWS\n",
    "    xlab = \"SON \" + var[2] + \" anomalies\"\n",
    "\n",
    "y = weights[0, :]  # SV1\n",
    "ax1.set_ylabel(var[0] + \" SV weights\")\n",
    "ax1.set_xlabel(xlab)\n",
    "ax1.plot(x, y, \"*\", color=color)\n",
    "ax1.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color=color)\n",
    "red_line = mlines.Line2D([], [], color=\"red\", marker=\"*\", label=\"SV1 weights ~ \" + xlab)\n",
    "ax1.legend(handles=[red_line], loc=\"lower right\")\n",
    "\n",
    "ax2 = ax1.twiny()  # instantiate a second axes that shares the same x-axis\n",
    "color = \"blue\"\n",
    "if tlat >= 0:\n",
    "    x = group_2[2, :]  # JJA TWS\n",
    "    xlab = \"JJA \" + var[2] + \" anomalies\"\n",
    "else:\n",
    "    x = group_2[1, :]  # MAM TWS\n",
    "    xlab = \"MAM \" + var[2] + \" anomalies\"\n",
    "\n",
    "y = weights[1, :]  # SV2\n",
    "ax2.set_xlabel(xlab)\n",
    "ax2.plot(x, y, \"*\", color=color)\n",
    "ax2.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color=color)\n",
    "ax2.tick_params(axis=\"x\")\n",
    "blue_line = mlines.Line2D(\n",
    "    [], [], color=\"blue\", marker=\"*\", label=\"SV2 weights ~ \" + xlab\n",
    ")\n",
    "ax2.legend(handles=[blue_line], loc=\"upper left\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of example code  \n",
    "\n",
    "# ------ Global SVD ------\n",
    "* Calculate climatology & anomalies, then\n",
    "* Detrend data & reshape for SVD & finally\n",
    "* Identify amplification vs. early onset vectors\n",
    "\n",
    "#### TODO, in the global code we'll need to: \n",
    "* Determine how SV weights correspond with GPP anomalies (+ or -)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use esmlab to calculate climatology & anomalies\n",
    "# TODO, refer to variables by name (e.g. anom_gpp) to improve readability\n",
    "# Will leave this for now to have a more generalizable code\n",
    "ds2 = ds.isel(time=slice(-months, None))  # Select last N years of data\n",
    "ds_clim = esmlab.core.climatology(ds2, freq=\"mon\")  # Calculate climatology\n",
    "ds_anom = esmlab.core.anomaly(\n",
    "    ds2, clim_freq=\"mon\", time_coord_name=\"time\"\n",
    ")  # not sure how to use slice_mon_clim_time\n",
    "ds_anom = ds_anom.where(\n",
    "    ds_anom.get(var[0]).max(dim=\"time\")\n",
    ")  # mask out regions with no GPP for all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define wrapper function that includes: \n",
    "- Detrending, reshaping, SVD & \n",
    "- Returns a np.array with vectors, weights, theta & varfrac\n",
    "- We dont' need the SV for TBOT and TWS, but we do need seasonal means of detrended anomalies "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to cacluate svd vectors, weights, etc.\n",
    "# First we have to detrend the data\n",
    "def svd_wrap(anom, years=years):\n",
    "    theta = np.full(years, np.nan)\n",
    "    varfrac = np.full(years, np.nan)\n",
    "    if np.isnan(anom)[0] == False:\n",
    "        # detrend results, if needed\n",
    "        detrend = signal.detrend(anom)\n",
    "\n",
    "        # reshape timeseries vector into a matrix (months x year)\n",
    "        matrix = np.zeros([12, years])\n",
    "        for iyr in range(years):\n",
    "            matrix[0:12, iyr] = detrend[iyr * 12 : (iyr + 1) * 12]\n",
    "\n",
    "        # Call the function to calculate the singular vectors and their annual weights\n",
    "        [vectors, weights] = svd.decompose(matrix)\n",
    "        [theta, varfrac] = svd.calc_redistribution(vectors, weights, matrix)\n",
    "\n",
    "    return theta, varfrac, vectors, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select a single grid point to work with first\n",
    "## Make sure wrapper results are the same as above\n",
    "# [theta_wrap, varfrac_wrap, vectors_wrap, weights_wrap] = svd_wrap(ds_temp.get(var[0]).values, years)\n",
    "# plt.plot(theta_wrap - theta, '*') ;\n",
    "# plt.plot(varfrac_wrap - varfrac, '*',c='r') ;\n",
    "# print ('differences between results look prety small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make frunction to Loop through each grid cell & dataset to save output\n",
    "- Would Dask make this faster?\n",
    "- how do we also mask non-veg land cells?\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, this is the time consuming bit of the code.\n",
    "# JH recommends using map_blocks, once it's developed farther\n",
    "def calc_svd(da, nyears=years):\n",
    "    dims = dict(zip(da.dims, da.shape))\n",
    "    yx_shape = (dims[\"lat\"], dims[\"lon\"])\n",
    "    # setup output variables\n",
    "    # number of vectors in svd = nyears\n",
    "    theta_shape = yx_shape + (nyears,)\n",
    "    vector_shape = yx_shape + (nyears, 12)\n",
    "    weights_shape = yx_shape + (nyears, nyears)\n",
    "    out = xr.Dataset(coords={\"lat\": da.lat, \"lon\": da.lon, \"sv\": range(nyears)})\n",
    "\n",
    "    varnames = [\"theta\", \"varfrac\", \"vectors\", \"weights\"]\n",
    "    out[\"theta\"] = xr.DataArray(np.zeros(theta_shape), dims=(\"lat\", \"lon\", \"sv\"))\n",
    "    out[\"varfrac\"] = xr.DataArray(np.zeros(theta_shape), dims=(\"lat\", \"lon\", \"sv\"))\n",
    "    out[\"vectors\"] = xr.DataArray(\n",
    "        np.zeros(vector_shape), dims=(\"lat\", \"lon\", \"sv\", \"time\")\n",
    "    )\n",
    "    out[\"weights\"] = xr.DataArray(\n",
    "        np.zeros(weights_shape), dims=(\"lat\", \"lon\", \"sv\", \"year\")\n",
    "    )\n",
    "    # For clarity, rename dimensions of weights (dim1 = vector#, dim2 = year)\n",
    "    #                                   vectors (dim1 = vector#, dim2 = month)\n",
    "    for (i, j), mask in np.ndenumerate(da.isel(time=0).isnull()):\n",
    "        if not mask:\n",
    "            return_vals = svd_wrap(da.isel(lat=i, lon=j).data)\n",
    "\n",
    "            for varname, vals in zip(varnames, return_vals):\n",
    "                out[varname][i, j, ...] = vals\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO, again more descriptive names will make the code easier to read, but for now keeping general\n",
    "# e.g. svd_gpp = calc_svd(ds_anom['GPP'])\n",
    "da0 = ds_anom.get(var[0]).transpose(\"time\", \"lat\", \"lon\")\n",
    "svd0 = calc_svd(da0)\n",
    "print(\"finished svd0, \" + var[0])\n",
    "\n",
    "# JH, When I loaded the data as you suggested in cell #2-3, a bunch of warnings crop up.\n",
    "# Reading the data less effiently doesn't generate the errors (as curently implimented),\n",
    "# and also seems to make the loop work more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange output & write out to scratch\n",
    "def transpose(da_in, var):\n",
    "    out = da_in\n",
    "    out[\"theta\"] = out.theta.transpose(\"sv\", \"lat\", \"lon\")\n",
    "    out[\"varfrac\"] = out.varfrac.transpose(\"sv\", \"lat\", \"lon\")\n",
    "    out[\"vectors\"] = out.vectors.transpose(\"sv\", \"time\", \"lat\", \"lon\")\n",
    "    out[\"weights\"] = out.weights.transpose(\"sv\", \"year\", \"lat\", \"lon\")\n",
    "    ds_clim[\"time\"] = out.time  # get time coords to match\n",
    "    out[\"climatology\"] = ds_clim.get(var)\n",
    "    # out.to_netcdf(path='/glade/scratch/wwieder/svd/'+model+'_svd_'+var+'.nc')\n",
    "    return out\n",
    "\n",
    "\n",
    "# this could also be more specific for readability, using 'GPP' instead of var[0]\n",
    "svd0 = transpose(svd0, var[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex over time for sv vectors & climatology for SH to match hydrologic year"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdSH = svd0.copy(deep=True)\n",
    "svdSH = svdSH.reindex(time=idx)\n",
    "svdSH[\"time\"] = svd0.time\n",
    "svd0[\"climatology\"] = svd0.climatology.where(svd0.lat >= 0, svdSH.climatology)\n",
    "svd0[\"vectors\"] = svd0.vectors.where(svd0.lat >= 0, svdSH.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd0.climatology.sel(lat=-5, lon=295, method=\"nearest\").plot()\n",
    "svd0.vectors.isel(sv=0).sel(lat=-5, lon=295, method=\"nearest\").plot(color=\"r\")\n",
    "svd0.vectors.isel(sv=1).sel(lat=-5, lon=295, method=\"nearest\").plot(color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Correlations between vectors and climatology of GPP ---\n",
    "Example from http://xarray.pydata.org/en/stable/dask.html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bottleneck\n",
    "\n",
    "# also try weighting correlation based on monthly contribution to annual flux\n",
    "# This is code that's likely better suited for a 3rd partly library like esmlab\n",
    "# TODO, file issue with requst for these kinds of statistical functions to esmlab?\n",
    "\n",
    "\n",
    "def covariance_gufunc(x, y):\n",
    "    return (\n",
    "        (x - x.mean(axis=-1, keepdims=True)) * (y - y.mean(axis=-1, keepdims=True))\n",
    "    ).mean(axis=-1)\n",
    "\n",
    "\n",
    "def pearson_correlation_gufunc(x, y):\n",
    "    return covariance_gufunc(x, y) / (x.std(axis=-1) * y.std(axis=-1))\n",
    "\n",
    "\n",
    "def pearson_correlation(x, y, dim):\n",
    "    return xr.apply_ufunc(\n",
    "        pearson_correlation_gufunc,\n",
    "        x,\n",
    "        y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float],\n",
    "    )\n",
    "\n",
    "\n",
    "# Weighted coorelation\n",
    "def covariance_gufunc_wgt(x, y, w):\n",
    "    return (\n",
    "        (w * x - (x * w).mean(axis=-1, keepdims=True))\n",
    "        * (y - (y * w).mean(axis=-1, keepdims=True))\n",
    "    ).mean(axis=-1)\n",
    "\n",
    "\n",
    "def pearson_correlation_gufunc_wgt(x, y, w):\n",
    "    return covariance_gufunc_wgt(x, y, w) / np.sqrt(\n",
    "        covariance_gufunc_wgt(x, x, w) * covariance_gufunc_wgt(y, y, w)\n",
    "    )\n",
    "\n",
    "\n",
    "def pearson_correlation_wgt(x, y, w, dim):\n",
    "    return xr.apply_ufunc(\n",
    "        pearson_correlation_gufunc_wgt,\n",
    "        x,\n",
    "        y,\n",
    "        w,\n",
    "        input_core_dims=[[dim], [dim], [dim]],\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float],\n",
    "    )\n",
    "\n",
    "\n",
    "# rank correlation\n",
    "def spearman_correlation_gufunc(x, y):\n",
    "    x_ranks = bottleneck.rankdata(x, axis=-1)\n",
    "    y_ranks = bottleneck.rankdata(y, axis=-1)\n",
    "    return pearson_correlation_gufunc(x_ranks, y_ranks)\n",
    "\n",
    "\n",
    "def spearman_correlation(x, y, dim):\n",
    "    return xr.apply_ufunc(\n",
    "        spearman_correlation_gufunc,\n",
    "        x,\n",
    "        y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull off climatology of GPP & calculate weights based on annual cycle\n",
    "print(svd0.time)\n",
    "GPPwgt = svd0.climatology / svd0.climatology.sum(dim=\"time\")\n",
    "svd0.climatology.sel(lat=tlat, lon=tlon, method=\"nearest\").plot()\n",
    "svd0.vectors.isel(sv=0).sel(lat=tlat, lon=tlon, method=\"nearest\").plot()\n",
    "svd0.vectors.isel(sv=1).sel(lat=tlat, lon=tlon, method=\"nearest\").plot()\n",
    "# GPPwgt.isel(time=0).plot(robust=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the correlations, GPP vectors w/ GPP climatology\n",
    "# pearson, weighted, and ranked correlations\n",
    "dims = dict(zip(ds[var[0]].dims, ds[var[0]].shape))\n",
    "yx_shape = (dims[\"lat\"], dims[\"lon\"])\n",
    "# setup output variables\n",
    "\n",
    "# number of vectors in svd = nyears\n",
    "corr_shape = (years,) + yx_shape\n",
    "\n",
    "GPP_corr = xr.Dataset(coords={\"sv\": range(years), \"lat\": ds.lat, \"lon\": ds.lon})\n",
    "GPP_corr[\"pearson\"] = xr.DataArray(np.zeros(corr_shape), dims=(\"sv\", \"lat\", \"lon\"))\n",
    "GPP_corr[\"weighted\"] = xr.DataArray(np.zeros(corr_shape), dims=(\"sv\", \"lat\", \"lon\"))\n",
    "GPP_corr[\"ranked\"] = xr.DataArray(np.zeros(corr_shape), dims=(\"sv\", \"lat\", \"lon\"))\n",
    "\n",
    "for i in range(years):\n",
    "    temp_vector = svd0.vectors.isel(sv=i)\n",
    "    GPP_corr.pearson[i, ...] = pearson_correlation(\n",
    "        svd0.climatology, temp_vector, \"time\"\n",
    "    )\n",
    "    GPP_corr.weighted[i, ...] = pearson_correlation_wgt(\n",
    "        svd0.climatology, temp_vector, GPPwgt, \"time\"\n",
    "    )\n",
    "    GPP_corr.ranked[i, ...] = spearman_correlation(\n",
    "        svd0.climatology, temp_vector, \"time\"\n",
    "    )\n",
    "\n",
    "GPP_corr.weighted[0, :, :].plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make plots to visualize results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a function for making panel plots of maps\n",
    "def map_function(da, cb=0, cmap=None, ax=None, title=None, vmax=None, vmin=None):\n",
    "    \"\"\"a function to make one subplot\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # da.plot(ax=ax)  # more custom args\n",
    "    im = ax.pcolormesh(\n",
    "        da.lon,\n",
    "        da.lat,\n",
    "        da.values,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmax=vmax,\n",
    "        vmin=vmin,\n",
    "        cmap=cmap,\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.coastlines()\n",
    "    ax.set_extent([-180, 180, -65, 80], crs=ccrs.PlateCarree())\n",
    "    # allows for different colrobars on each plot\n",
    "    if cb == 1:\n",
    "        fig.colorbar(im, ax=ax, shrink=0.40, pad=0, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the map_function\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=2,\n",
    "    figsize=(14, 6),\n",
    "    constrained_layout=True,\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    ")\n",
    "\n",
    "for index, ax in np.ndenumerate(axes):\n",
    "    # there are various ways to do this part, index in this case is a tuple (ie `(0, 0)`)\n",
    "    da = svd0.varfrac.isel(sv=index[0])\n",
    "    map_function(\n",
    "        da,\n",
    "        ax=ax,\n",
    "        title=\"SV\" + str(index[0]) + \" fraction of variance\",\n",
    "        vmax=da.max(),\n",
    "        vmin=da.min(),\n",
    "        cmap=\"gist_earth_r\",\n",
    "        cb=1,\n",
    "    )\n",
    "\n",
    "# or to provide a common color bar, set vmax/min so same values and\n",
    "# fig.colorbar(im,ax=axes.ravel().tolist(), shrink=0.5);\n",
    "plt.savefig(\"/glade/scratch/wwieder/svd/\" + model + \"_\" + var[0] + \"_sv_Variance.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask values to isolate regions with amplificaiton vs. phase shift (early green-up)?\n",
    "# here using rank correlation, but also could use weighted\n",
    "mask_shape = (2,) + yx_shape\n",
    "sv_mask = xr.Dataset(\n",
    "    coords={\"sv_type\": [\"amplification\", \"phase shift\"], \"lat\": ds.lat, \"lon\": ds.lon}\n",
    ")\n",
    "sv_mask[\"ranked\"] = xr.DataArray(np.zeros(mask_shape), dims=(\"sv_type\", \"lat\", \"lon\"))\n",
    "sv_mask[\"ranked\"][0, ...] = xr.ufuncs.fabs(GPP_corr.ranked[0, ...]) >= xr.ufuncs.fabs(\n",
    "    GPP_corr.ranked[1, ...]\n",
    ")\n",
    "sv_mask[\"ranked\"][1, ...] = xr.ufuncs.fabs(GPP_corr.ranked[1, ...]) > xr.ufuncs.fabs(\n",
    "    GPP_corr.ranked[0, ...]\n",
    ")\n",
    "\n",
    "sv_mask[\"weighted\"] = xr.DataArray(np.zeros(mask_shape), dims=(\"sv_type\", \"lat\", \"lon\"))\n",
    "sv_mask[\"weighted\"][0, ...] = xr.ufuncs.fabs(\n",
    "    GPP_corr.weighted[0, ...]\n",
    ") >= xr.ufuncs.fabs(GPP_corr.weighted[1, ...])\n",
    "sv_mask[\"weighted\"][1, ...] = xr.ufuncs.fabs(\n",
    "    GPP_corr.weighted[1, ...]\n",
    ") > xr.ufuncs.fabs(GPP_corr.weighted[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=2,\n",
    "    figsize=(14, 6),\n",
    "    constrained_layout=True,\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    ")\n",
    "\n",
    "for index, ax in np.ndenumerate(axes):\n",
    "    # there are various ways to do this part, index in this case is a tuple (ie `(0, 0)`)\n",
    "    da = sv_mask.ranked.isel(sv_type=index[0])\n",
    "    map_function(da, ax=ax, title=sv_mask.sv_type[index[0]].values, cmap=\"gist_earth_r\")\n",
    "plt.savefig(\"/glade/scratch/wwieder/svd/\" + model + \"_\" + var[0] + \"_vectors.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase shift mainly in agricutrulal regions & temperate decid. forest!"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPP_corr.weighted.isel(sv=0).plot(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This selects amplification vectors\n",
    "flip_corr = 0 + (GPP_corr.ranked < 0.0)\n",
    "flip_corr[0, :, :] = flip_corr[0, :, :].where(\n",
    "    sv_mask.ranked.isel(sv_type=0), 0\n",
    ")  # only reverse amplitude vectors where needed\n",
    "flip_corr[1, :, :] = flip_corr[1, :, :].where(sv_mask.ranked.isel(sv_type=1), 0)\n",
    "# flip_corr.isel(sv=0).plot()\n",
    "\n",
    "# Still need a better way to think about flipping early phase vectors where needed...\n",
    "max_vector = svd0.vectors.argmax(dim=\"time\")\n",
    "min_vector = svd0.vectors.argmin(dim=\"time\")\n",
    "flip_vector = 0 + (min_vector < max_vector)\n",
    "flip_vector[0, :, :] = flip_vector[0, :, :].where(sv_mask.ranked.isel(sv_type=1), 0)\n",
    "flip_vector[1, :, :] = flip_vector[1, :, :].where(sv_mask.ranked.isel(sv_type=0), 0)\n",
    "\n",
    "flip_combined = flip_corr + flip_vector\n",
    "flip_combined.isel(sv=0).plot()\n",
    "\n",
    "svd2 = svd0.copy(deep=True)  ## create new datasets that don't change the orig. results\n",
    "for i in range(years):\n",
    "    svd2.vectors[i, ...] = svd2.vectors[i, ...].where(\n",
    "        flip_combined[i, ...] <= 0, -1 * svd2.vectors[i, ...]\n",
    "    )\n",
    "    svd2.weights[i, ...] = svd2.weights[i, ...].where(\n",
    "        flip_combined[i, ...] <= 0, -1 * svd2.weights[i, ...]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one more logical if if amplication vectors and mean vectors < 0 fip vectors & weights [maybe again...]\n",
    "flip_amp = svd2.vectors.mean(dim=\"time\") < 0.0\n",
    "flip_amp[0, :, :] = flip_amp[0, :, :].where(sv_mask.ranked.isel(sv_type=0), 0)\n",
    "flip_amp[1, :, :] = flip_amp[1, :, :].where(sv_mask.ranked.isel(sv_type=1), 0)\n",
    "flip_amp.isel(sv=0).plot(robust=True)\n",
    "for i in range(years):\n",
    "    svd2.vectors[i, ...] = svd2.vectors[i, ...].where(\n",
    "        flip_amp[i, ...] <= 0, -1 * svd2.vectors[i, ...]\n",
    "    )\n",
    "    svd2.weights[i, ...] = svd2.weights[i, ...].where(\n",
    "        flip_amp[i, ...] <= 0, -1 * svd2.weights[i, ...]\n",
    "    )\n",
    "\n",
    "# Now repeat the correlation between vectors and climatology\n",
    "for i in range(years):\n",
    "    temp_vector = svd2.vectors.isel(sv=i)\n",
    "    GPP_corr.pearson[i, ...] = pearson_correlation(\n",
    "        svd0.climatology, temp_vector, \"time\"\n",
    "    )\n",
    "    GPP_corr.weighted[i, ...] = pearson_correlation_wgt(\n",
    "        svd0.climatology, temp_vector, GPPwgt, \"time\"\n",
    "    )\n",
    "    GPP_corr.ranked[i, ...] = spearman_correlation(\n",
    "        svd0.climatology, temp_vector, \"time\"\n",
    "    )\n",
    "\n",
    "# GPP_corr.weighted[0,:,:].plot(robust=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This seems to work for amplitude vectors & early phase vectors in SH and NH\n",
    "#### Requires reindexing time vectors for SH climatology & vectors to reflect hydrologic cycle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot First and Second Singular Vectors\n",
    "# Made this long plotting routine into a function\n",
    "\n",
    "\n",
    "def monthly_plots(ds_all, ds0, ds1, a, b, nA, nB, multi=0.5e4):\n",
    "\n",
    "    fig = plt.figure(4, figsize=(6, 4.5))\n",
    "    barwidth = 0.2\n",
    "    month = np.arange(12)\n",
    "    multi = multi  # This is a totally arbitrary scale factor, decided by me, to make figures look good!\n",
    "    ymax = np.max(ds0.vectors.values)\n",
    "\n",
    "    # --- GPP, SVD 1 & 2 for regions in phase with GPP\n",
    "    vectors2plot = ds0.vectors * multi\n",
    "    mm2plot = ds_all.climatology\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "    if b > 0:\n",
    "        ax1.set_xticklabels(\n",
    "            [\"Jan\", \"\", \"\", \"Apr\", \"\", \"\", \"Jul\", \"\", \"\", \"Oct\", \"\", \"\"]\n",
    "        )\n",
    "    else:\n",
    "        ax1.set_xticklabels(\n",
    "            [\"Jul\", \"\", \"\", \"Oct\", \"\", \"\", \"Jan\", \"\", \"\", \"Apr\", \"\", \"\"]\n",
    "        )\n",
    "\n",
    "    ax1.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        mm2plot / (4 * max(mm2plot)) - 0.05,\n",
    "        \"darkgray\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    ax1.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        mm2plot / (4 * max(mm2plot)) + 0.35,\n",
    "        \"darkgray\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "\n",
    "    ax1.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        vectors2plot[0, :] * 0.75 + 0.35,\n",
    "        \"b-\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    ax1.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        vectors2plot[1, :] * 0.75 - 0.05,\n",
    "        \"r-\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    ax1.axhline(-0.05, color=\"black\")\n",
    "    ax1.axhline(0.35, color=\"black\")\n",
    "    ax1.set_xticks(np.array(month) + 2 * barwidth)\n",
    "    ax1.set_yticks([-0.05, 0.35])\n",
    "    ax1.set_yticklabels([\"SV2\", \"SV1\"])\n",
    "    ax1.set_ylim([-0.2, 0.65])\n",
    "\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.52,\n",
    "        str(int(ds0.varfrac[0] * 100))\n",
    "        + \"%\\n$\\Theta$=\"\n",
    "        + str(\"%.2f\" % ds0.theta[0])\n",
    "        + \"\\nn=\"\n",
    "        + str(nA),\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.15,\n",
    "        str(int(ds0.varfrac[1] * 100)) + \"%\\n$\\Theta$=\" + str(\"%.2f\" % ds0.theta[1]),\n",
    "    )\n",
    "    ax1.title.set_text(var[0] + \" \" + str(a) + \"-\" + str(b) + \", amplification\")\n",
    "    ax1.text(4, -0.29, \"Month\")\n",
    "\n",
    "    # ----- Make second panel, where SV1 = phase shift ----\n",
    "    vectors2plot = ds1.vectors * multi\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    if b > 0:\n",
    "        ax2.set_xticklabels(\n",
    "            [\"Jan\", \"\", \"\", \"Apr\", \"\", \"\", \"Jul\", \"\", \"\", \"Oct\", \"\", \"\"]\n",
    "        )\n",
    "    else:\n",
    "        ax2.set_xticklabels(\n",
    "            [\"Jul\", \"\", \"\", \"Oct\", \"\", \"\", \"Jan\", \"\", \"\", \"Apr\", \"\", \"\"]\n",
    "        )\n",
    "\n",
    "    ax2.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        mm2plot / (4 * max(mm2plot)) - 0.05,\n",
    "        \"darkgray\",\n",
    "        linewidth=3,\n",
    "    )  # ,alpha=0.3)\n",
    "    ax2.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        mm2plot / (4 * max(mm2plot)) + 0.35,\n",
    "        \"darkgray\",\n",
    "        linewidth=3,\n",
    "    )  # ,alpha=0.3)\n",
    "\n",
    "    ax2.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        vectors2plot[0, :] * 0.75 + 0.35,\n",
    "        \"b-\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    ax2.plot(\n",
    "        np.array(month) + 2 * barwidth,\n",
    "        vectors2plot[1, :] * 0.75 - 0.05,\n",
    "        \"r-\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    ax2.axhline(-0.05, color=\"black\")\n",
    "    ax2.axhline(0.35, color=\"black\")\n",
    "    ax2.set_xticks(np.array(month) + 2 * barwidth)\n",
    "    ax2.set_yticks([-0.05, 0.35])\n",
    "    ax2.set_yticklabels([\"SV2\", \"SV1\"])\n",
    "    ax2.set_ylim([-0.2, 0.65])\n",
    "\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.52,\n",
    "        str(int(ds1.varfrac[0] * 100))\n",
    "        + \"%\\n$\\Theta$=\"\n",
    "        + str(\"%.2f\" % ds1.theta[0])\n",
    "        + \"\\nn=\"\n",
    "        + str(nB),\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.15,\n",
    "        str(int(ds1.varfrac[1] * 100)) + \"%\\n$\\Theta$=\" + str(\"%.2f\" % ds1.theta[1]),\n",
    "    )\n",
    "    ax2.title.set_text(var[0] + \" \" + str(a) + \"-\" + str(b) + \", early phase\")\n",
    "    ax2.text(4, -0.29, \"Month\")\n",
    "\n",
    "    plt.savefig(\n",
    "        \"/glade/scratch/wwieder/svd/\"\n",
    "        + model\n",
    "        + \"_SV_\"\n",
    "        + var[0]\n",
    "        + \"_lat_\"\n",
    "        + str(a)\n",
    "        + \"-\"\n",
    "        + str(b)\n",
    "        + \".eps\"\n",
    "    )\n",
    "    ax1.remove()\n",
    "    ax2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean annual cycle and mean SV1 and SV2 for different regions\n",
    "# TODO, JH, How do I save plots for each lat band invidually?\n",
    "# It would be nice to put these all in one figure & or visualize plots here\n",
    "\n",
    "a = [-30, 0, 30, 55]  # Min lat, -20 to 10 for Amazon\n",
    "b = [0, 30, 55, 80]  # Max lat\n",
    "c = 0  # min lon, 280-310 for the Amazon\n",
    "d = 360  # max lon\n",
    "\n",
    "for i in range(len(a)):\n",
    "    svd_plot = svd2\n",
    "    mean_svd_plot = svd_plot.sel(lat=slice(a[i], b[i]), lon=slice(c, d)).mean(\n",
    "        (\"lat\", \"lon\")\n",
    "    )\n",
    "    # -- Amplitude vector --\n",
    "    mean_svd_plotA = (\n",
    "        svd_plot.where(sv_mask.ranked.isel(sv_type=0))\n",
    "        .sel(lat=slice(a[i], b[i]), lon=slice(c, d))\n",
    "        .mean((\"lat\", \"lon\"))\n",
    "    )\n",
    "    countSetA = sv_mask.ranked.isel(sv_type=0).sel(\n",
    "        lat=slice(a[i], b[i]), lon=slice(c, d)\n",
    "    )\n",
    "    nA = countSetA.where(countSetA > 0).count().values\n",
    "    # -- Phase shift vector --\n",
    "    mean_svd_plotB = (\n",
    "        svd_plot.where(sv_mask.ranked.isel(sv_type=1))\n",
    "        .sel(lat=slice(a[i], b[i]), lon=slice(c, d))\n",
    "        .mean((\"lat\", \"lon\"))\n",
    "    )\n",
    "    countSetB = sv_mask.ranked.isel(sv_type=1).sel(\n",
    "        lat=slice(a[i], b[i]), lon=slice(c, d)\n",
    "    )\n",
    "    nB = countSetB.where(countSetB > 0).count().values\n",
    "\n",
    "    monthly_plots(\n",
    "        mean_svd_plot, mean_svd_plotA, mean_svd_plotB, a=a[i], b=b[i], nA=nA, nB=nB\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems to:\n",
    "* Appropriately identify SV1 as amplification or early phase vectors\n",
    "* SV1 explians most of varriation (at least for high & mid latitudes)\n",
    "* SV2 of phase shift = amplificaiton!\n",
    "* maybe additional work needed for tropics?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Correlate SV weights w/ seasonal anomalies -----\n",
    "* Claculate seasonal means of detrended data\n",
    "* Reshape anomaly vectors to season x year array\n",
    "* Correlate GPP SV1 weights w/ seasonal GPP anomalies & \n",
    "* Reverse / flip weights where needed\n",
    "* Calculate correlation for GPP SV1 weights w/ seaonal climate anomalies"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtrend(anom, dim):\n",
    "    # note: apply always moves core dimensions to the end\n",
    "    return xr.apply_ufunc(\n",
    "        signal.detrend,\n",
    "        anom,\n",
    "        input_core_dims=[[dim]],\n",
    "        output_core_dims=[[dim]],\n",
    "        kwargs={\"axis\": -1},\n",
    "    )\n",
    "\n",
    "\n",
    "# Build datastet to hold results\n",
    "dims = dict(zip(ds_anom.get(var[0]).dims, ds_anom.get(var[0]).shape))\n",
    "out_shape = (dims[\"lat\"], dims[\"lon\"], 4, years)\n",
    "seasonID = xr.DataArray([12, 3, 6, 9], dims=\"season\")  # seasonal time slices\n",
    "seasonName = xr.DataArray(\n",
    "    [\"DJF\", \"MAM\", \"JJA\", \"SON\"], dims=\"season\"\n",
    ")  # can coords be a string?\n",
    "yearID = xr.DataArray(np.unique(ds_anom.time[\"time.year\"]), dims=\"year\")\n",
    "dt_anom = xr.Dataset(\n",
    "    coords={\n",
    "        \"lat\": ds_anom.lat,\n",
    "        \"lon\": ds_anom.lon,\n",
    "        \"season\": seasonName,\n",
    "        \"year\": yearID,\n",
    "    }\n",
    ")\n",
    "\n",
    "for v in range(len(var)):\n",
    "    dt_anom[var[v]] = xr.DataArray(\n",
    "        np.zeros(out_shape), dims=(\"lat\", \"lon\", \"season\", \"year\")\n",
    "    )\n",
    "    dt = dtrend(ds_anom.get(var[v]).load().fillna(0), \"time\")\n",
    "    dt_season = dt.resample(time=\"QS-DEC\").mean().isel(time=slice(0, -1))\n",
    "    # now reshape\n",
    "    for m in range(len(seasonID)):\n",
    "        temp = dt_season.where(dt_season.time[\"time.month\"] == seasonID[m], drop=True)\n",
    "        dt_anom[var[v]][:, :, m, :] = temp\n",
    "\n",
    "dt_anom = dt_anom.transpose(\"season\", \"year\", \"lat\", \"lon\")\n",
    "dt_anom[var[0]].isel(season=2).mean(dim=\"year\").plot(robust=True)\n",
    "dt_anomSD = dt_anom.std(\n",
    "    dim=\"year\"\n",
    ")  # calculates SD across years for each season of 25 y anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_anomSD[var[0]].isel(season=2).plot(robust=True)\n",
    "print(dt_anomSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results look at example from single grid cell, with\n",
    "# GPP_sv weights ~ seasonal anomalies\n",
    "# sv1 Early shift = spring & sv2 Amplificaton = summer\n",
    "\n",
    "tlat = -5\n",
    "tlon = 300\n",
    "if tlat >= 0:\n",
    "    plt.plot(\n",
    "        dt_anom[var[0]].sel(lat=tlat, lon=tlon, method=\"nearest\").isel(season=1),\n",
    "        svd2.weights.sel(lat=tlat, lon=tlon, method=\"nearest\").isel(sv=0),\n",
    "        \"*\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        dt_anom[var[0]].sel(lat=tlat, lon=tlon, method=\"nearest\").isel(season=2),\n",
    "        svd2.weights.sel(lat=tlat, lon=tlon, method=\"nearest\").isel(sv=1),\n",
    "        \"*\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        labels=(\"SV1 weights ~ MAM GPP anomalies\", \"SV2 weights ~ JJA GPP anomalies\"),\n",
    "    )\n",
    "else:\n",
    "    plt.plot(\n",
    "        dt_anom[var[0]].sel(lat=tlat, lon=tlon, method=\"nearest\").isel(season=3),\n",
    "        svd2.weights.sel(lat=tlat, lon=tlon, method=\"nearest\").isel(sv=0),\n",
    "        \"*\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        dt_anom[var[0]].sel(lat=tlat, lon=tlon, method=\"nearest\").isel(season=1),\n",
    "        svd2.weights.sel(lat=tlat, lon=tlon, method=\"nearest\").isel(sv=1),\n",
    "        \"*\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        labels=(\"SV1 weights ~ SON GPP anomalies\", \"SV2 weights ~ MAM GPP anomalies\"),\n",
    "    )\n",
    "\n",
    "plt.xlabel(var[0] + \" seasonal anomalies (gC/m2/s)\")\n",
    "plt.ylabel(var[0] + \" SV weights\")\n",
    "print(svd2.sel(lat=tlat, lon=tlon, method=\"nearest\").isel(sv=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights typically have have the same sign as GPP anomalies\n",
    "* But not always in tropics\n",
    "* Do we need to reverse vectors and weights where this negative correlation exists?\n",
    "* We'll try this in a bit"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look for correlations for GPP sv weights & GPP anomalies\n",
    "* this will be used to flip weights for sv1 as needed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out correlation matrix for weights of GPP SV with detrended anomalies\n",
    "# Loop over each SV (2), variable (3), & season (4)\n",
    "\n",
    "\n",
    "def corr_wrap(anom, svd):\n",
    "    da = anom.to_array()\n",
    "    dims = dict(zip(da.dims, da.shape))\n",
    "    corr_shape = (2, dims[\"season\"], dims[\"lat\"], dims[\"lon\"])\n",
    "\n",
    "    # setup output variables\n",
    "    corr = xr.Dataset(\n",
    "        coords={\"sv\": range(2), \"season\": seasonName, \"lat\": ds.lat, \"lon\": ds.lon}\n",
    "    )\n",
    "\n",
    "    for v in range(len(var)):  # variables\n",
    "        corr[var[v]] = xr.DataArray(\n",
    "            np.zeros(corr_shape), dims=(\"sv\", \"season\", \"lat\", \"lon\")\n",
    "        )\n",
    "\n",
    "        # loop through to calculate correlation coef.\n",
    "        for i in range(2):  # vectors\n",
    "            for j in range(len(seasonName)):  # seasons\n",
    "                return_vals = pearson_correlation(\n",
    "                    anom.get(var[v]).isel(season=j), svd[\"weights\"].isel(sv=i), \"year\"\n",
    "                )\n",
    "                corr[var[v]][i, j, ...] = return_vals\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "# first make year dimensions indential\n",
    "svd0[\"year\"] = dt_anom.get(\"year\")\n",
    "svd2[\"year\"] = dt_anom.get(\"year\")\n",
    "svd0_corr = corr_wrap(dt_anom, svd0)\n",
    "svd2_corr = corr_wrap(dt_anom, svd2)\n",
    "\n",
    "# --- Check that results ar sensible ---\n",
    "# GPP SV1 coorelated with JJA GPP anomalies in NH, as expected\n",
    "# GPP SV1 coorelated with MAM GPP anomalies in early shift regions (NE USA + Ag.)\n",
    "svd2_corr[var[0]].isel(sv=0, season=3).plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights do not necessarily have the same sign as GPP anomalies\n",
    "(e.g. see Amazon in plots above & below)\n",
    "- Itentify season with maximum abs(correlation) between SV weights & GPP anomalies\n",
    "- Reverse vectors and weights where this value < 0\n",
    "- repeat correlation matrix"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr.ufuncs.fabs calculates the absolute value for an xarray object\n",
    "# argmax picks the index of the largest value across a dimension.\n",
    "\n",
    "gppMask = svd0_corr[var[0]].max(dim=\"season\")\n",
    "maxGPP_season = (\n",
    "    xr.ufuncs.fabs(svd0_corr[var[0]].fillna(-1))\n",
    "    .argmax(dim=\"season\")\n",
    "    .where(gppMask.notnull())\n",
    ")\n",
    "maxGPP_season.isel(sv=0).plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next should look at environmental correlations in dominant seasons\n",
    "cor.svd[weights] ~ TBOT and TWS anomalies\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simplify correlations to look at values only when\n",
    "# weights from SV most strongly correlate with seasonal GPP anomalies\n",
    "\n",
    "# 1) set up dataset to hold results\n",
    "max_season_corr = xr.Dataset(coords={\"sv\": range(years), \"lat\": ds.lat, \"lon\": ds.lon})\n",
    "max_season_SD = xr.Dataset(coords={\"lat\": ds.lat, \"lon\": ds.lon})\n",
    "\n",
    "for i in range(len(var)):\n",
    "    max_season_corr[var[i]] = xr.DataArray(\n",
    "        np.zeros(corr_shape), dims=(\"sv\", \"lat\", \"lon\")\n",
    "    )\n",
    "    max_season_SD[var[i]] = xr.DataArray(np.zeros(yx_shape), dims=(\"lat\", \"lon\"))\n",
    "\n",
    "print(max_season_SD)\n",
    "# 2) loop through vectors and seasons\n",
    "# write out corr coefficients where maxGPP_seaon == T\n",
    "for i in range(2):  # vectors\n",
    "    for j in range(len(seasonName)):  # seasons\n",
    "        for k in range(len(var)):\n",
    "            max_season_corr.get(var[k])[i, ...] = svd2_corr.get(var[k])[\n",
    "                i, j, ...\n",
    "            ].where(maxGPP_season[i, ...] == j, max_season_corr.get(var[k])[i, ...])\n",
    "            max_season_SD[var[k]] = dt_anomSD.get(var[k])[j, ...].where(\n",
    "                maxGPP_season[0, ...] == j, max_season_SD.get(var[k])\n",
    "            )\n",
    "\n",
    "max_season_SD[var[0]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xarray facet plots for pannels\n",
    "# each row show correlations between SV_1 weights and seasonal anomalies\n",
    "for k in range(len(var)):\n",
    "    simple = (\n",
    "        svd2_corr[var[k]].isel(sv=0).plot(x=\"lon\", y=\"lat\", col=\"season\", col_wrap=4)\n",
    "    )\n",
    "\n",
    "plt.savefig(\"/glade/scratch/wwieder/svd/\" + model + \"_\" + var[k] + \"multi_summary.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_anomSD)\n",
    "print(max_season_SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- first add season of max correlation of SV weights with GPP anomalies to dataset being plotted\n",
    "max_season_corr[\"maxGPP_season\"] = maxGPP_season\n",
    "max_season_SD[\"maxGPP_season\"] = maxGPP_season.isel(sv=0)\n",
    "var2 = [\"maxGPP_season\"] + var\n",
    "# Now call the map_function\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "    figsize=(16, 8),\n",
    "    constrained_layout=True,\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    ")\n",
    "\n",
    "i = 0\n",
    "for index, ax in np.ndenumerate(axes):\n",
    "    # there are various ways to do this part, index in this case is a tuple (ie `(0, 0)`)\n",
    "    da = max_season_corr[var2[i]].isel(sv=0)\n",
    "    if i == 0:\n",
    "        # Can I ake these arguments a list or a dictionary?\n",
    "        cmap = plt.get_cmap(\"viridis\", 4)\n",
    "        vmax = 3.5\n",
    "        vmin = -0.5\n",
    "        title = \"month max correlation SVD1 weights & \" + var[0] + \" anomalies\"\n",
    "    else:\n",
    "        cmap = \"RdBu_r\"\n",
    "        vmax = 1\n",
    "        vmin = -1\n",
    "        title = \"correlation SV1 Weights ~ seasonal \" + var2[i] + \" anomalies\"\n",
    "    map_function(da, ax=ax, title=title, cmap=cmap, vmax=vmax, vmin=vmin, cb=1)\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "# or to provide a common color bar, set vmax/min so same values and\n",
    "# fig.colorbar(im,ax=axes.ravel().tolist(), shrink=0.5);\n",
    "plt.savefig(\"/glade/scratch/wwieder/svd/\" + model + \"_\" + var[0] + \"_summary.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SD of GPP anomalies in season when max correlation between SV1 & GPP anomalies\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "    figsize=(16, 8),\n",
    "    constrained_layout=True,\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    ")\n",
    "\n",
    "i = 0\n",
    "maxVals = [3.5, 3.0e-5, 2.5, 200]  # arbitrary values for min/max\n",
    "minVals = [-0.5, 0, 0, 0]\n",
    "for index, ax in np.ndenumerate(axes):\n",
    "    # there are various ways to do this part, index in this case is a tuple (ie `(0, 0)`)\n",
    "    if i == 0:\n",
    "        da = max_season_SD[var2[i]]  # .where(max_season_corr[var2[i]].isel(sv=0))\n",
    "        # Can I ake these arguments a list or a dictionary?\n",
    "        cmap = plt.get_cmap(\"viridis\", 4)\n",
    "        # cmap = colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "        title = \"month max correlation SVD1 weights & \" + var[0] + \" anomalies\"\n",
    "    else:\n",
    "        da = max_season_SD[var2[i]].where(max_season_corr[var2[i]].isel(sv=0))\n",
    "        cmap = \"viridis\"\n",
    "        title = model + \" SD \" + var2[i] + \" seasonal anomalies\"\n",
    "    vmax = maxVals[i]\n",
    "    vmin = minVals[i]\n",
    "    map_function(da, ax=ax, title=title, cmap=cmap, vmax=vmax, vmin=vmin, cb=1)\n",
    "    i = i + 1\n",
    "\n",
    "plt.savefig(\"/glade/scratch/wwieder/svd/\" + model + \"_SD_anomalies.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grids characterized by early phase SV1 also have higher % varriation explianted by soil water SV"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the map_function\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "    figsize=(16, 8),\n",
    "    constrained_layout=True,\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    ")\n",
    "\n",
    "i = 0\n",
    "for index, ax in np.ndenumerate(axes):\n",
    "    # there are various ways to do this part, index in this case is a tuple (ie `(0, 0)`)\n",
    "    da = max_season_corr[var2[i]].isel(sv=0).where(sv_mask.ranked.isel(sv_type=0))\n",
    "    if i == 0:\n",
    "        # Can I ake these arguments a list or a dictionary?\n",
    "        cmap = plt.get_cmap(\"viridis\", 4)\n",
    "        # cmap = colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "        vmax = 3.5\n",
    "        vmin = -0.5\n",
    "        title = \"month max correlation SVD1 weights & \" + var[0] + \" anomalies\"\n",
    "    else:\n",
    "        cmap = \"RdBu_r\"\n",
    "        vmax = 1\n",
    "        vmin = -1\n",
    "        title = \"correlation SV1 Weights ~ seasonal \" + var2[i] + \" anomalies\"\n",
    "    map_function(da, ax=ax, title=title, cmap=cmap, vmax=vmax, vmin=vmin, cb=1)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the map_function\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "    figsize=(16, 8),\n",
    "    constrained_layout=True,\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    ")\n",
    "\n",
    "i = 0\n",
    "for index, ax in np.ndenumerate(axes):\n",
    "    # there are various ways to do this part, index in this case is a tuple (ie `(0, 0)`)\n",
    "    da = max_season_corr[var2[i]].isel(sv=0).where(sv_mask.ranked.isel(sv_type=1))\n",
    "    if i == 0:\n",
    "        # Can I ake these arguments a list or a dictionary?\n",
    "        cmap = plt.get_cmap(\"viridis\", 4)\n",
    "        # cmap = colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "        vmax = 3.5\n",
    "        vmin = -0.5\n",
    "        title = \"month max correlation SVD1 weights & \" + var[0] + \" anomalies\"\n",
    "    else:\n",
    "        cmap = \"RdBu_r\"\n",
    "        vmax = 1\n",
    "        vmin = -1\n",
    "        title = \"correlation SV1 Weights ~ seasonal \" + var2[i] + \" anomalies\"\n",
    "    map_function(da, ax=ax, title=title, cmap=cmap, vmax=vmax, vmin=vmin, cb=1)\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-python-tutorial]",
   "language": "python",
   "name": "conda-env-miniconda3-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
